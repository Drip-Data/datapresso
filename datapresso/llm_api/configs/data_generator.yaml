# Datapresso LLM API Configuration Template
# -----------------------------------------
# This template is used to generate default configuration files for different
# Datapresso stages (e.g., data_generator, quality_assessor) within the
# `datapresso/datapresso/llm_api/configs/` directory.
#
# Users should edit the generated YAML files in the `configs/` directory,
# NOT this template directly.
#
# It is STRONGLY recommended to use environment variables for API keys
# using the `env(VAR_NAME)` syntax.

# Default provider to use for this stage if not explicitly specified in the API call.
# Must match one of the keys under the 'providers' section below.
default_provider: openai

# Global settings applied to all providers unless overridden in a specific provider's config.
global_settings:
  max_retries: 3          # Number of retries on API call failure.
  retry_delay: 1.0        # Initial delay in seconds before the first retry (exponential backoff is used).
  timeout: 120            # Timeout in seconds for API requests.
  # temperature: 0.7      # Example: You could set a global default temperature here.
  # max_tokens: 2048      # Example: You could set a global default max_tokens here.

# Provider configurations section.
# Define each LLM service provider you want this stage to be able to use.
providers:

  # --- Official Cloud Providers ---
  openai:
    provider_type: openai
    api_key: env(OPENAI_API_KEY) # Recommended: Reads OPENAI_API_KEY from environment variables.
    model: gpt-4-turbo           # Default model for this provider.
    temperature: 0.7
    max_tokens: 4096
    # cost_per_1k_tokens: ... # Optional override

  anthropic:
    provider_type: anthropic
    api_key: env(ANTHROPIC_API_KEY)
    model: claude-3-opus-20240229
    temperature: 0.7
    max_tokens: 4096

  gemini:
    provider_type: gemini
    api_key: env(GOOGLE_API_KEY)
    model: gemini-1.5-pro-latest
    temperature: 0.8
    max_tokens: 8192

  deepseek:
    provider_type: deepseek
    api_key: env(DEEPSEEK_API_KEY)
    model: deepseek-chat
    temperature: 0.7
    max_tokens: 4096

  # --- Custom / Local Providers ---
  # Example: Custom OpenAI-compatible API (e.g., OpenRouter, local vLLM/TGI)
  my_custom_openai_endpoint:
    provider_type: generic_openai
    api_base: http://localhost:8000/v1 # REQUIRED: The base URL.
    # api_key: env(MY_CUSTOM_KEY)      # Optional: API key if needed.
    model: mistralai/Mixtral-8x7B-Instruct-v0.1 # REQUIRED: Model name expected by endpoint.
    temperature: 0.6
    max_tokens: 4000

  # Example: Local model via Hugging Face transformers
  local_llama3_8b:
    provider_type: local
    model_path: /path/to/your/Llama-3-8B-Instruct # REQUIRED: Path to model directory.
    device: cuda         # Optional: 'cuda' or 'cpu'.
    dtype: float16       # Optional: 'float16' or 'float32'.
    temperature: 0.6
    max_tokens: 2048

# --- System Prompt Templates (Optional) ---
# Define reusable system prompts for this stage. Referenced by name.
system_prompt_templates:
  # 默认通用助手
  default: "You are a helpful AI assistant."

  # --- Prompts for Data Generation Tasks ---

  # 基于种子生成新的指令和响应
  generate_from_seed: |
    You are an expert data generator. Your task is to create a new, high-quality instruction-response pair.
    This new pair should be similar in style, topic, and complexity to the provided seed example, but distinct in its specific content.
    Ensure the new instruction is clear, specific, and answerable.
    Ensure the new response is accurate, detailed (including rationale or step-by-step thinking if appropriate, similar to the seed), and directly addresses the *new* instruction you generated.
    Avoid directly copying phrases from the seed example unless necessary for context.

    Seed Example:
    Instruction: {seed_instruction}
    Response: {seed_response_text} # Or potentially include rationale/final_answer depending on generation strategy

    Generate the new instruction and response pair, and format the output as a JSON object that strictly adheres to the following schema:

    ```json
    {
      "instruction": "...",
      "response": {
        "origin_text": "...",
        "rationale": "...",
        "final_answer": "..."
      }
    }
    ```
    Ensure that the response includes a well-reasoned rationale (if applicable) and a concise final answer (if applicable).
    Only output the JSON object. Do not include any surrounding text or explanations.

  # 提升种子指令的复杂度
  increase_complexity: |
    You are an expert data generator. Given the following seed instruction, create a new instruction that is conceptually similar but significantly more complex or requires deeper reasoning. The new instruction should be clear and solvable.

    Seed Instruction: {seed_instruction}

    Generate the more complex instruction.

  # 改变种子指令的领域或上下文
  change_context: |
    You are an expert data generator. Adapt the following seed instruction to a different context or domain (e.g., change from physics to biology, or from formal to informal tone). The core task or question type should remain similar if possible, but applied to the new context. The new instruction must be clear and answerable.

    Seed Instruction: {seed_instruction}
    Target Context/Domain: {target_context} # This would be provided dynamically

    Generate the adapted instruction for the target context/domain.

  # 仅生成与种子指令相似的新指令
  generate_similar_instruction: |
    You are an expert data generator. Based on the provided seed instruction, generate a new, distinct instruction that covers a similar topic or requires a similar skill, but asks a different specific question.

    Seed Instruction: {seed_instruction}

    Generate only the new instruction.

# --- Output Schema Templates (Optional) ---
# Define reusable JSON schemas for structured output for this stage. Referenced by name.
output_schema_templates:
  person_info:
    type: object
    properties:
      name: { type: string, description: "Full name of the person mentioned." }
      age: { type: integer, description: "Age of the person, if mentioned." }
      city: { type: string, description: "City of residence, if mentioned." }
      profession: { type: string, description: "Profession or job title, if mentioned."}
    required: [name]

  code_review:
    type: object
    properties:
      has_issues: { type: boolean, description: "Does the code snippet have potential issues (e.g., bugs, style problems, inefficiencies)?" }
      issues_found: { type: array, items: { type: string }, description: "List of specific issues identified in the code." }
      suggestions: { type: array, items: { type: string }, description: "List of concrete suggestions for improving the code." }
      overall_assessment: { type: string, description: "A brief summary of the code's quality and purpose." }
    required: [has_issues, overall_assessment]

  quality_score:
    type: object
    properties:
      clarity: { type: number, description: "Score from 1 (low) to 5 (high) for clarity." }
      coherence: { type: number, description: "Score from 1 (low) to 5 (high) for coherence." }
      accuracy: { type: number, description: "Score from 1 (low) to 5 (high) for accuracy relative to any source/context." }
      relevance: { type: number, description: "Score from 1 (low) to 5 (high) for relevance to the prompt/task." }
      justification: { type: string, description: "Brief explanation for the scores given."}
    required: [clarity, coherence, accuracy, relevance, justification]

  # Schema for generating structured instruction/response pairs
  generated_instruction_response:
    type: object
    properties:
      instruction:
        type: string
        description: "The newly generated, clear, and specific instruction."
      response:
        type: object
        description: "The response corresponding to the generated instruction."
        properties:
          origin_text:
            type: string
            description: "The primary text content of the response."
          rationale:
            type: string
            description: "(Optional) Step-by-step reasoning, explanation, or thought process behind the response."
          final_answer:
            type: string
            description: "(Optional) A concise final answer extracted from the response, if applicable (e.g., for math, multiple choice)."
        required: [origin_text] # Require at least the main response text
    required: [instruction, response]