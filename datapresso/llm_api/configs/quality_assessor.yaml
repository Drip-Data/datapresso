# Datapresso LLM API Configuration Template
# -----------------------------------------
# This template is used to generate default configuration files for different
# Datapresso stages (e.g., data_generator, quality_assessor) within the
# `datapresso/datapresso/llm_api/configs/` directory.
#
# Users should edit the generated YAML files in the `configs/` directory,
# NOT this template directly.
#
# It is STRONGLY recommended to use environment variables for API keys
# using the `env(VAR_NAME)` syntax.

# Default provider to use for this stage if not explicitly specified in the API call.
# Must match one of the keys under the 'providers' section below.
default_provider: openai

# Global settings applied to all providers unless overridden in a specific provider's config.
global_settings:
  max_retries: 3          # Number of retries on API call failure.
  retry_delay: 1.0        # Initial delay in seconds before the first retry (exponential backoff is used).
  timeout: 120            # Timeout in seconds for API requests.
  # temperature: 0.7      # Example: You could set a global default temperature here.
  # max_tokens: 2048      # Example: You could set a global default max_tokens here.

# Provider configurations section.
# Define each LLM service provider you want this stage to be able to use.
providers:

  # --- Official Cloud Providers ---
  openai:
    provider_type: openai
    api_key: env(OPENAI_API_KEY) # Recommended: Reads OPENAI_API_KEY from environment variables.
    model: gpt-4-turbo           # Default model for this provider.
    temperature: 0.7
    max_tokens: 4096
    # cost_per_1k_tokens: ... # Optional override

  anthropic:
    provider_type: anthropic
    api_key: env(ANTHROPIC_API_KEY)
    model: claude-3-opus-20240229
    temperature: 0.7
    max_tokens: 4096

  gemini:
    provider_type: gemini
    api_key: env(GOOGLE_API_KEY)
    model: gemini-1.5-pro-latest
    temperature: 0.8
    max_tokens: 8192

  deepseek:
    provider_type: deepseek
    api_key: env(DEEPSEEK_API_KEY)
    model: deepseek-chat
    temperature: 0.7
    max_tokens: 4096

  # --- Custom / Local Providers ---
  # Example: Custom OpenAI-compatible API (e.g., OpenRouter, local vLLM/TGI)
  my_custom_openai_endpoint:
    provider_type: generic_openai
    api_base: http://localhost:8000/v1 # REQUIRED: The base URL.
    # api_key: env(MY_CUSTOM_KEY)      # Optional: API key if needed.
    model: mistralai/Mixtral-8x7B-Instruct-v0.1 # REQUIRED: Model name expected by endpoint.
    temperature: 0.6
    max_tokens: 4000

  # Example: Local model via Hugging Face transformers
  local_llama3_8b:
    provider_type: local
    model_path: /path/to/your/Llama-3-8B-Instruct # REQUIRED: Path to model directory.
    device: cuda         # Optional: 'cuda' or 'cpu'.
    dtype: float16       # Optional: 'float16' or 'float32'.
    temperature: 0.6
    max_tokens: 2048

# --- System Prompt Templates (Optional) ---
# Define reusable system prompts for this stage. Referenced by name.
system_prompt_templates:
  # 默认通用助手
  default: "You are a helpful AI assistant."

  # --- Prompts for Quality Assessment Tasks ---

  # 通用质量评估 (配合 quality_score schema)
  quality_assessor_general: |
    You are an expert evaluator. Assess the quality of the provided instruction-response pair based on the following criteria:
    - Clarity: Is the instruction clear and unambiguous? Is the response easy to understand?
    - Coherence: Does the response logically follow the instruction? Is the reasoning (if any) sound and well-structured?
    - Accuracy: Is the information provided in the response factually correct and accurate according to the instruction? (If external knowledge is required, assess based on general knowledge unless stated otherwise).
    - Relevance: Does the response directly and fully address the instruction without unnecessary information?
    Provide a score from 1 (very poor) to 5 (excellent) for each criterion. Also, provide a brief justification for your scores, highlighting specific strengths or weaknesses.
    Output your assessment in the specified JSON format.

  # 指令复杂度评估 (可能需要不同的 schema)
  instruction_complexity_assessor: |
    You are an expert evaluator. Analyze the complexity of the provided instruction. Consider factors like:
    - Required knowledge domain and depth.
    - Number of constraints or steps involved.
    - Ambiguity or need for clarification.
    - Cognitive load required to understand and answer.
    Assign a complexity score (e.g., 1-5 or 0.0-1.0) and provide a brief justification. Output in the specified JSON format.

  # 响应质量评估 (侧重于响应本身，可能需要不同的 schema)
  response_quality_assessor: |
    You are an expert evaluator. Assess the quality of the provided response *relative to the instruction*. Consider:
    - Correctness and factual accuracy.
    - Completeness in addressing all parts of the instruction.
    - Clarity and readability of the language.
    - Appropriateness of tone and style.
    - Quality of reasoning or explanation (if applicable).
    Assign relevant scores and provide justification. Output in the specified JSON format.

  # 安全性评估 (可能需要不同的 schema 或特定模型)
  safety_assessor: |
    You are a safety evaluator. Analyze the provided instruction and response for any potentially harmful, unethical, biased, or inappropriate content. Consider categories like hate speech, harassment, dangerous content, misinformation, etc.
    Provide a safety score (e.g., 0.0 for harmful, 1.0 for safe) and identify any specific safety concerns found. Output in the specified JSON format.

# --- Output Schema Templates (Optional) ---
# Define reusable JSON schemas for structured output for this stage. Referenced by name.
output_schema_templates:
  person_info:
    type: object
    properties:
      name: { type: string, description: "Full name of the person mentioned." }
      age: { type: integer, description: "Age of the person, if mentioned." }
      city: { type: string, description: "City of residence, if mentioned." }
      profession: { type: string, description: "Profession or job title, if mentioned."}
    required: [name]

  code_review:
    type: object
    properties:
      has_issues: { type: boolean, description: "Does the code snippet have potential issues (e.g., bugs, style problems, inefficiencies)?" }
      issues_found: { type: array, items: { type: string }, description: "List of specific issues identified in the code." }
      suggestions: { type: array, items: { type: string }, description: "List of concrete suggestions for improving the code." }
      overall_assessment: { type: string, description: "A brief summary of the code's quality and purpose." }
    required: [has_issues, overall_assessment]

  quality_score:
    type: object
    properties:
      clarity: { type: number, description: "Score from 1 (low) to 5 (high) for clarity." }
      coherence: { type: number, description: "Score from 1 (low) to 5 (high) for coherence." }
      accuracy: { type: number, description: "Score from 1 (low) to 5 (high) for accuracy relative to any source/context." }
      relevance: { type: number, description: "Score from 1 (low) to 5 (high) for relevance to the prompt/task." }
      justification: { type: string, description: "Brief explanation for the scores given."}
    required: [clarity, coherence, accuracy, relevance, justification]