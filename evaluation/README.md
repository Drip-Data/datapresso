# 评估模块 (Evaluation Module)

本目录 (`evaluation/`) 主要用于存放与 **下游任务模型评估** 相关的代码和脚本。其核心目标是衡量使用 Datapresso 生成的 LIMO 数据集在实际模型微调或推理任务上的效果。

## 与 `quality_assessment` 的区别

*   **`quality_assessment/`**: 关注 **数据本身** 的质量。它在数据生成后、过滤前（或过滤中）运行，评估单个数据点或数据集的内在属性，如相关性、流畅性、安全性、事实准确性等，通常使用基于规则、基于模型（如 NLI）或基于 LLM 的方法进行打分。其目的是为了 **筛选** 出高质量的数据。
*   **`evaluation/`**: 关注 **使用数据后模型的效果**。它通常在数据过滤完成、得到最终的 LIMO 数据集之后运行。其目的是通过在标准基准测试集或特定下游任务上评估用 LIMO 数据微调（或 few-shot）的模型的性能（如准确率、F1 分数、BLEU 分数等），来 **验证** Datapresso 生成的数据集的有效性。

## 可能包含的内容

*   **评估脚本 (`run_evaluation.py`, `evaluate_downstream_task.py`)**: 用于加载 LIMO 数据集、加载预训练模型、使用 LIMO 数据进行微调或设置 few-shot 提示、在标准评估集上运行模型并计算指标的脚本。
*   **指标计算模块 (`metrics.py`)**: 实现特定任务所需的评估指标。
*   **模型适配代码 (`model_wrappers.py`)**: 可能需要一些包装代码来适配不同的模型库（如 Hugging Face Transformers, PEFT）和评估流程。
*   **评估数据集处理 (`eval_data_loader.py`)**: 加载和预处理用于模型评估的标准基准数据集。
*   **配置文件 (`eval_config.yaml`)**: 用于配置评估流程的参数，如模型路径、评估数据集、批处理大小、评估指标等。
*   **结果存储与分析**: 可能包含用于保存和分析评估结果的工具或脚本。

## 集成点

*   **Pipeline 集成**: `datapresso.Pipeline` 可以选择性地在最后阶段触发此处的评估脚本。
*   **LlamaFactory 集成**: 如果使用 `datapresso/llamafactory/` 进行微调，评估模块可以与其配合，加载微调后的模型进行评估。
*   **独立运行**: 评估脚本通常也可以独立运行，方便针对不同的 LIMO 数据集或模型进行快速实验。

## 当前状态

(根据实际情况填写，例如：此模块目前为占位符，等待后续开发 / 已包含初步的评估脚本...)